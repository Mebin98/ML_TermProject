# -*- coding: utf-8 -*-
"""Clarans_Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s9gTNgOkUuKoSTrLonZHWdFs6zRsOTWY
"""

# !pip install pyclustering
import numpy as np  # NumPy is used for numerical operations and array manipulation.
import pandas as pd  # Pandas is used for data manipulation and analysis.
import matplotlib.pyplot as plt  # Matplotlib is used for creating plots and visualizations.
from sklearn.decomposition import PCA  # PCA (Principal Component Analysis) is used for dimensionality reduction.
from sklearn.metrics import silhouette_samples, silhouette_score  # These functions are used for silhouette analysis.
import matplotlib.cm as cm  # Matplotlib colormap for visualizations.
from pyclustering.cluster.clarans import clarans  # PyClustering library for CLARANS clustering algorithm.
from pyclustering.utils import timedcall  # PyClustering utility for measuring execution time.


def load_dataset():
    # Loading datasets (paths may vary)
    data_std = pd.read_csv('/content/gdrive/My Drive/ML_PHW1/Project/K-means/scaled_data_standard.csv')
    data_minmax = pd.read_csv('/content/gdrive/My Drive/ML_PHW1/Project/K-means/scaled_data_minmax.csv')
    data_normalizer = pd.read_csv('/content/gdrive/My Drive/ML_PHW1/Project/K-means/scaled_data_normalizer.csv')
    data_quantile = pd.read_csv('/content/gdrive/My Drive/ML_PHW1/Project/K-means/scaled_data_quantile.csv')
    data_robust = pd.read_csv('/content/gdrive/My Drive/ML_PHW1/Project/K-means/scaled_data_robust.csv')
    return data_std, data_minmax, data_normalizer, data_quantile, data_robust

def plot_clusters(data_pca, labels):
    """Plots clusters on PCA-reduced data."""
    unique_clusters = np.unique(labels)
    plt.figure(figsize=(8, 6))
    for cluster in unique_clusters:
        plt.scatter(data_pca[labels == cluster, 0], data_pca[labels == cluster, 1], label=f'Cluster {cluster}')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('Clusters in PCA-reduced Space')
    plt.legend()
    plt.show()

def plot_silhouette(data, cluster_labels):
    """Plots silhouette scores for each cluster."""
    silhouette_avg = silhouette_score(data, cluster_labels)
    print(f"The average silhouette_score is : {silhouette_avg}")
    sample_silhouette_values = silhouette_samples(data, cluster_labels)
    y_lower = 10
    for i in np.unique(cluster_labels):
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        color = cm.nipy_spectral(float(i) / len(np.unique(cluster_labels)))
        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)
        y_lower = y_upper + 10
    plt.title("The silhouette plot for the various clusters.")
    plt.xlabel("The silhouette coefficient values")
    plt.ylabel("Cluster label")
    plt.axvline(x=silhouette_avg, color="red", linestyle="--")
    plt.yticks([])
    plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    plt.show()

def cluster_data_then_evaluate_with_clarans(data, num_clusters=3, numlocal=5, maxneighbor=8):
    """Clusters data using CLARANS and evaluates the results."""

    # Preparing data for CLARANS
    data_list = data.tolist()  # directly convert the NumPy array to a list

    # CLARANS algorithm
    clarans_instance = clarans(data_list, num_clusters, numlocal, maxneighbor)

    # Running the algorithm and timing it
    (ticks, result) = timedcall(clarans_instance.process)

    # Retrieving clusters and medoids
    clusters = clarans_instance.get_clusters()
    medoids = clarans_instance.get_medoids()

    # Converting clusters to label format for evaluation
    labels = np.zeros(len(data))
    for cluster_id, cluster in enumerate(clusters):
        for index in cluster:
            labels[index] = cluster_id

    # Plotting and evaluation
    plot_clusters(data, labels)
    if len(np.unique(labels)) > 1:
        plot_silhouette(data, labels)
    else:
        print("Not enough clusters for silhouette analysis.")

def simulate_all_data_clarans():
    std, minmax, normal, quantile, robust = load_dataset()
    datasets = [std, minmax, normal, quantile, robust]
    for dataset in datasets:
        # Check and remove 'id' column if it exists
        if 'id' in dataset.columns:
            dataset = dataset.drop('id', axis=1)

        # Applying PCA for visualization
        pca = PCA(n_components=2)
        data_pca = pca.fit_transform(dataset)

        # Clustering and evaluation with CLARANS
        cluster_data_then_evaluate_with_clarans(data_pca)

simulate_all_data_clarans()